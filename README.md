# kaiburr
kaiburr ml task for Consumer Complaint Analysis

#dataseet link:https://files.consumerfinance.gov/ccdb/complaints.csv.zip
# üè¶ Banking Product Complaint Classification using Explainable AI (XAI)

## üìò Project Overview
This project aims to classify **consumer complaints** related to various **banking and financial products** using **Natural Language Processing (NLP)** and **Machine Learning/Deep Learning** models.  
The system also incorporates **Explainable AI (XAI)** techniques like **LIME** and **SHAP** to interpret and visualize model decisions, providing transparency in financial complaint classification.

---

## üéØ Objective
To build an AI-based text classification system that can:
- Automatically categorize customer complaints into appropriate product types (e.g., *Credit card*, *Mortgage*, *Loan*, etc.).
- Provide explainable insights into **why** a specific complaint was classified under a particular product.

---

## üìÇ Dataset Information

**Dataset Name:** Consumer Complaint Database  
**Source:** [U.S. Consumer Financial Protection Bureau (CFPB)](https://www.consumerfinance.gov/data-research/consumer-complaints/)  

### Dataset Description

| Column | Description |
|--------|--------------|
| `consumer_complaint_narrative` | Text description of the customer's complaint |
| `product` | The financial product/service category (Target variable) |
| `company`, `state`, `submitted_via` | Additional metadata (optional) |

---

## üßπ Data Preprocessing
The data undergoes multiple cleaning and preprocessing steps:

1. **Data Cleaning:**
   - Dropped missing or null entries
   - Selected only `consumer_complaint_narrative` and `product`

2. **Text Preprocessing:**
   - Tokenization  
   - Stopword removal  
   - Lemmatization  
   - Lowercasing and punctuation removal

3. **Feature Extraction:**
   - **TF-IDF (Term Frequency‚ÄìInverse Document Frequency)** for traditional models
   - **Word Embeddings (Word2Vec/BERT)** for deep learning models

---

## ‚öôÔ∏è Model Development

### üß† Machine Learning Models
- Logistic Regression  
- Naive Bayes  
- Random Forest  
- Support Vector Machine (SVM)

### ü§ñ Deep Learning Model (Proposed)
- **Architecture:** Bidirectional LSTM / BERT Classifier  
- **Embedding Layer:** Pre-trained Word Embeddings  
- **Loss Function:** CrossEntropyLoss  
- **Optimizer:** Adam  
- **Evaluation Metrics:** Accuracy, Loss, Error Rate, Precision, Recall, F1-Score

---

## üìà Model Evaluation Metrics
| Metric | Description |
|--------|--------------|
| **Train Accuracy** | Model performance on training data |
| **Validation Accuracy** | Model generalization on unseen data |
| **Error Rate** | 1 - Accuracy |
| **Loss** | Cross-entropy loss during optimization |
| **Execution Time** | Time taken per epoch/iteration |
| **Confusion Matrix** | Visualization of true vs predicted classes |
| **Classification Report** | Precision, Recall, F1-Score per class |

---

## üß© Explainable AI (XAI) Integration
To make the model interpretable and trustworthy:

- **LIME (Local Interpretable Model-Agnostic Explanations):**
  Highlights important words contributing to classification.

- **SHAP (SHapley Additive Explanations):**
  Shows feature importance and contribution to model prediction.

### Example:
